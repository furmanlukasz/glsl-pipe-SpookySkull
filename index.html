<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, shrink-to-fit=0, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
    <title>glsl-pipeline-SpookySkull</title>
    <link rel="stylesheet" href="styles.css">
  </head>
  
  <body> 
    <button id="speakButton">Ask Skull</button>
    <!-- <button id="playButton">PlayðŸŽ¶</button> -->

    <div id="messageListening" style="display:none;">Listening to your message...</div>
    <div id="messageOverlay" style="display:none;">ðŸŽƒ Sending your message to the afterlife will take ~30s ðŸŽƒ</div>
    <div id="threejs-container" style="width: 100%; height: 100%;"></div>

    <script type="importmap">
        {
          "imports": {
            "three": "https://unpkg.com/three@0.157.0/build/three.module.js",
            "three/addons/": "https://unpkg.com/three@0.157.0/examples/jsm/"
          }
        }
    </script>
    <script type="module">
      import { WebGLRenderer, PerspectiveCamera, Scene, BoxGeometry, ShaderMaterial, Mesh, Vector2, Vector3, Object3D, AudioLoader, Audio, AudioListener } from 'three';
      // import resolveLygia from "https://lygia.xyz/resolve.esm.js";
      import { resolveLygia } from 'resolve-lygia';
      // import { OBJLoader } from 'three/examples/jsm/loaders/OBJLoader';
      import { OBJLoader } from 'three/addons/loaders/OBJLoader.js';

      import { GlslPipeline } from './index.js';
      import { LLMEndpoint } from './llm-endpoint.js';

      let animation;

        // Speech recognition setup
      function startListening() {
        if (recognition && recognition.state !== "recording") {
            console.log("Listening...");
            recognition.start();
            document.getElementById("messageOverlay").style.display = "none";
            document.getElementById("messageListening").style.display = "block";
        }
        }
        
        const speakButton = document.getElementById('speakButton');
        let recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition || window.mozSpeechRecognition || window.msSpeechRecognition)();
        recognition.lang = 'en-US';
        recognition.interimResults = false;
        recognition.maxAlternatives = 5;

        speakButton.addEventListener('click', startListening);

        recognition.onresult = function(event) {
        // Hide the message overlay
        document.getElementById("messageListening").style.display = "none";
        // Show the message overlay
        document.getElementById("messageOverlay").style.display = "block";
        const speech = event.results[0][0].transcript;

        
        console.log("Recognized Speech:", speech);
        // Show the message overlay
        
        LLMEndpoint.callEndpoint(speech)
            .then(formattedOutput => {
                if (formattedOutput) {  // Check if the formattedOutput is not null
                    let utterance = new SpeechSynthesisUtterance(formattedOutput);
                    utterance.lang = "en-US";  // Set language to English
                    utterance.pitch = 0.4;  // Lower the pitch (default is 1, range is 0 to 2)
                    utterance.rate = 0.8;   // Make it slower (default is 1, range is 0.1 to 10)    
                    window.speechSynthesis.speak(utterance);
                }
            })
            .catch(error => {
                console.error("Error fetching or processing endpoint response:", error);
            });
        };

        recognition.onerror = function(event) {
        console.error("Speech recognition error:", event.error);
        };

      // Show the message overlay
      document.getElementById("messageOverlay").style.display = "block";

      // Mediapipe
	  import vision from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0';
      const { FaceLandmarker, FilesetResolver } = vision;
        // MediaPipe
      const filesetResolver = await FilesetResolver.forVisionTasks(
            'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm'
      );
      const faceLandmarker = await FaceLandmarker.createFromOptions( filesetResolver, {
            baseOptions: {
                modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task',
                delegate: 'GPU'
            },
            outputFaceBlendshapes: true,
            outputFacialTransformationMatrixes: true,
            runningMode: 'VIDEO',
            numFaces: 1
      } );
      if ( navigator.mediaDevices && navigator.mediaDevices.getUserMedia ) {
            navigator.mediaDevices.getUserMedia( { video: { facingMode: 'user' } } )
                .then( function ( stream ) {

                    video.srcObject = stream;
                    video.play();
                } )
                .catch( function ( error ) {
                    console.error( 'Unable to access the camera/webcam.', error );
                } );
      }

      let W = window,
          D = document;

      let width = W.innerWidth;
      let height = W.innerHeight;
      let pixelRatio = W.devicePixelRatio;
      const container = document.getElementById('threejs-container');
    //   let width = container.clientWidth;
    //   let height = container.clientHeight;

      const renderer = new WebGLRenderer();
      renderer.setPixelRatio(pixelRatio);
      renderer.setSize(width, height);
    //   D.body.appendChild(renderer.domElement);

    container.appendChild(renderer.domElement);

      const video = document.createElement( 'video' );
      video.addEventListener('play', function() {
        if (animation) {
      animation();
      }
      });

  // Fetch the shaders and then initialize the pipeline
  Promise.all([
    fetch('shaders/shader.vert').then(response => response.text()),
    fetch('shaders/shader.frag').then(response => response.text())
  ]).then(([vertexShaderCode, fragmentShaderCode]) => {
    // Initialize GLSL Buffers with the fetched shaders
    const glsl_sandbox = new GlslPipeline(renderer);
    glsl_sandbox.load(resolveLygia(fragmentShaderCode), resolveLygia(vertexShaderCode));


      const mesh = new Mesh(new BoxGeometry(1, 1, 1), glsl_sandbox.material);
      const scene = new Scene();
      const cam = new PerspectiveCamera(45, width / height, 0.001, 200);
      cam.position.z = 3;
    //   scene.add(mesh);
    

      const objLoader = new OBJLoader();
            objLoader.load('assets/skull.obj', (object) => {
                object.traverse((child) => {
                    if (child instanceof Mesh) {
                        child.material = glsl_sandbox.branchMaterial("OBJMATERIAL");
                        child.castShadow = true;
                        child.receiveShadow = true;
                    }
                });
                // Move the skull bit back and up a bit
                object.position.set(0, 0.0, 0);
                object.scale.set(0.55, 0.55, 0.55);
                // Add name to the object so we can find it later
                object.name = 'Skull';
                scene.add(object);
            });

      function lerp(a, b, t) {
            return (1 - t) * a + t * b;
      }         

      const transform = new Object3D();

      // Now assign the defined function to 'animation'
      animation = () => {
      // Your animation logic here
        requestAnimationFrame(animation); // Keep this line to create an animation loop


      if (video.readyState >= HTMLMediaElement.HAVE_METADATA) {

            const results = faceLandmarker.detectForVideo(video, Date.now());

            if (results.facialTransformationMatrixes.length > 0) {

                const facialTransformationMatrixes = results.facialTransformationMatrixes[0].data;

                transform.matrix.fromArray(facialTransformationMatrixes);
                transform.matrix.decompose(transform.position, transform.quaternion, transform.scale);

                // Update the position and rotation of the object
                const objectToFollow = scene.getObjectByName('Skull');

                let t = 0.1; // Interpolation factor, between 0 and 1
                objectToFollow.rotation.x = lerp(objectToFollow.rotation.x, transform.rotation.x, t);
                objectToFollow.rotation.y = lerp(objectToFollow.rotation.y, -transform.rotation.y, t);
                objectToFollow.rotation.z = lerp(objectToFollow.rotation.z, -transform.rotation.z, t);

                objectToFollow.position.x = lerp(objectToFollow.position.x, -transform.position.x * 0.01, t);
                objectToFollow.position.y = lerp(objectToFollow.position.y, transform.position.y * 0.01, t) + 0.02;
                objectToFollow.position.z = lerp(objectToFollow.position.z, -transform.position.z * 0.02, t);

            }
          // Initialize the animation loop
          animation();
         }
      }

    // Load the audio and set it to loop
    const listener = new AudioListener();
    const audioLoader = new AudioLoader();
    audioLoader.load('assets/Spook by PeriTune.mp3', function (buffer) {
        const sound = new Audio(listener);
        sound.setBuffer(buffer);
        sound.setLoop(true);
        // Set the volume to 0.5
        sound.setVolume(0.0);
        sound.play();
    });


      const draw = () => {
          glsl_sandbox.renderScene(scene, cam);
          requestAnimationFrame(draw);
      };

      const resize = () => {
          width = W.innerWidth;
          height = W.innerHeight;
          pixelRatio = W.devicePixelRatio;

          renderer.setPixelRatio(pixelRatio);
          renderer.setSize(width, height);

          glsl_sandbox.setSize(width, height);

          cam.aspect = width / height;
          cam.updateProjectionMatrix();
      };

      W.addEventListener("resize", resize);
      resize();

      draw();
    });
    </script>
  </body>
</html>